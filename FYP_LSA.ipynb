{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FYP_LSA.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lakshika-Swarnamali/Diagrams/blob/master/FYP_LSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiSdDcfPBrfA"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use ('fivethirtyeight')\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import tweepy as tw\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import networkx\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "sns.set(font_scale=1.5)\n",
        "sns.set_style(\"whitegrid\")\n",
        "# packages to store and manipulate data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# plotting packages\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# model building package\n",
        "import sklearn\n",
        "\n",
        "# package to clean text\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "-r3gK2MyBzV8",
        "outputId": "a2e11933-9845-4395-f99a-42a826d61979"
      },
      "source": [
        "log = pd.read_csv(\"https://raw.githubusercontent.com/GihanKLG/TwitterCovid19Dataset/master/2020-10-12.csv\",encoding='latin1')\n",
        "log[(log['lang'] == 'en')].to_csv(\"out.csv\", index=False)\n",
        "log = pd.read_csv('out.csv', lineterminator='\\n',encoding='latin1')\n",
        "log.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coordinates</th>\n",
              "      <th>created_at</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>media</th>\n",
              "      <th>urls</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>id</th>\n",
              "      <th>in_reply_to_screen_name</th>\n",
              "      <th>in_reply_to_status_id</th>\n",
              "      <th>in_reply_to_user_id</th>\n",
              "      <th>lang</th>\n",
              "      <th>place</th>\n",
              "      <th>possibly_sensitive</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>retweet_id</th>\n",
              "      <th>retweet_screen_name</th>\n",
              "      <th>source</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_url</th>\n",
              "      <th>user_created_at</th>\n",
              "      <th>user_screen_name</th>\n",
              "      <th>user_default_profile_image</th>\n",
              "      <th>user_description</th>\n",
              "      <th>user_favourites_count</th>\n",
              "      <th>user_followers_count</th>\n",
              "      <th>user_friends_count</th>\n",
              "      <th>user_listed_count</th>\n",
              "      <th>user_location</th>\n",
              "      <th>user_name</th>\n",
              "      <th>user_screen_name.1</th>\n",
              "      <th>user_statuses_count</th>\n",
              "      <th>user_time_zone</th>\n",
              "      <th>user_urls</th>\n",
              "      <th>user_verified</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Sun Oct 11 23:59:54 +0000 2020</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1315442032651927552</td>\n",
              "      <td>BalutSmithy</td>\n",
              "      <td>1.315403e+18</td>\n",
              "      <td>1.296441e+18</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>@BalutSmithy @StephenJohn59 @jaketapper @kaitl...</td>\n",
              "      <td>https://twitter.com/NorthLapp/status/131544203...</td>\n",
              "      <td>Sun Mar 25 19:25:10 +0000 2012</td>\n",
              "      <td>NorthLapp</td>\n",
              "      <td>False</td>\n",
              "      <td>I need more followers, I will expose the truth...</td>\n",
              "      <td>1979</td>\n",
              "      <td>1291</td>\n",
              "      <td>1805</td>\n",
              "      <td>28</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Renee J North Lapp</td>\n",
              "      <td>NorthLapp</td>\n",
              "      <td>12503</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Sun Oct 11 23:59:56 +0000 2020</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1315442037286678529</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>1.315348e+18</td>\n",
              "      <td>PetraCEsser1</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>RT @PetraCEsser1: @HSRetoucher Thank you. I am...</td>\n",
              "      <td>https://twitter.com/johncoppa44/status/1315442...</td>\n",
              "      <td>Fri Feb 26 09:25:23 +0000 2016</td>\n",
              "      <td>johncoppa44</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3031</td>\n",
              "      <td>31</td>\n",
              "      <td>287</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>john coppa</td>\n",
              "      <td>johncoppa44</td>\n",
              "      <td>7490</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Sun Oct 11 23:59:56 +0000 2020</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://twitter.com/onlinelonghorn/status/1315...</td>\n",
              "      <td>0</td>\n",
              "      <td>1315442039870324736</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>*corona liked this post* https://t.co/3mAPNWNAfK</td>\n",
              "      <td>https://twitter.com/ivythekingrat/status/13154...</td>\n",
              "      <td>Mon Dec 11 16:09:37 +0000 2017</td>\n",
              "      <td>ivythekingrat</td>\n",
              "      <td>False</td>\n",
              "      <td>nope</td>\n",
              "      <td>4001</td>\n",
              "      <td>95</td>\n",
              "      <td>153</td>\n",
              "      <td>0</td>\n",
              "      <td>puerto rico</td>\n",
              "      <td>rat zone</td>\n",
              "      <td>ivythekingrat</td>\n",
              "      <td>1633</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Sun Oct 11 23:59:55 +0000 2020</td>\n",
              "      <td>SidNaaz</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1315442036028260353</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>120</td>\n",
              "      <td>1.315334e+18</td>\n",
              "      <td>SidNaaz13Dstny</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>RT @SidNaaz13Dstny: Points\\n\\n1. Salman Sir #S...</td>\n",
              "      <td>https://twitter.com/Tejakshi1/status/131544203...</td>\n",
              "      <td>Sat Jan 04 06:50:54 +0000 2020</td>\n",
              "      <td>Tejakshi1</td>\n",
              "      <td>False</td>\n",
              "      <td>Ã°ÂÂ¤Â¡</td>\n",
              "      <td>57815</td>\n",
              "      <td>282</td>\n",
              "      <td>579</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tejakshi shetty</td>\n",
              "      <td>Tejakshi1</td>\n",
              "      <td>39117</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Sun Oct 11 23:59:55 +0000 2020</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1315442033331507203</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10711</td>\n",
              "      <td>1.315345e+18</td>\n",
              "      <td>CLewandowski_</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/#!/download/ipad\" ...</td>\n",
              "      <td>RT @CLewandowski_: WHO now advising against lo...</td>\n",
              "      <td>https://twitter.com/LoisR52511816/status/13154...</td>\n",
              "      <td>Sun Aug 02 16:10:47 +0000 2020</td>\n",
              "      <td>LoisR52511816</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>28886</td>\n",
              "      <td>88</td>\n",
              "      <td>277</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>LoisR</td>\n",
              "      <td>LoisR52511816</td>\n",
              "      <td>13602</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  coordinates                      created_at  ... user_urls user_verified\n",
              "0         NaN  Sun Oct 11 23:59:54 +0000 2020  ...       NaN         False\n",
              "1         NaN  Sun Oct 11 23:59:56 +0000 2020  ...       NaN         False\n",
              "2         NaN  Sun Oct 11 23:59:56 +0000 2020  ...       NaN         False\n",
              "3         NaN  Sun Oct 11 23:59:55 +0000 2020  ...       NaN         False\n",
              "4         NaN  Sun Oct 11 23:59:55 +0000 2020  ...       NaN         False\n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOUbiHeKB5Ib",
        "outputId": "8e28e598-7b03-4519-a800-1a84bae4b4ce"
      },
      "source": [
        "# importing pandas package\n",
        "import pandas as pd\n",
        " \n",
        "# making data frame from csv file\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/GihanKLG/TwitterCovid19Dataset/master/2020-10-12.csv\", index_col =\"id\")\n",
        " \n",
        "# retrieving columns by indexing operator\n",
        "# first = data[\"user_name\"]\n",
        "my_column_names = [ 'user_name','text']\n",
        "df = pd.DataFrame(data=data, columns=my_column_names)\n",
        "lda = pd.DataFrame(data=data, columns=my_column_names)\n",
        "lsa = pd.DataFrame(data=data, columns=my_column_names)\n",
        "\n",
        "# print(first)\n",
        "# print(first['id', 'user_name','retweet_count','retweet_id'])\n",
        "print(df)\n",
        "# df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                              user_name                                               text\n",
            "id                                                                                        \n",
            "1315442032651927552  Renee J North Lapp  @BalutSmithy @StephenJohn59 @jaketapper @kaitl...\n",
            "1315442033507684352             🎃 leh 🎃  RT @dougraz: todo dia pensando será se eu ja p...\n",
            "1315442037286678529          john coppa  RT @PetraCEsser1: @HSRetoucher Thank you. I am...\n",
            "1315442040172425216      Reivindicación               @PedroAzzola https://t.co/RxL0DAaQaI\n",
            "1315442039870324736            rat zone   *corona liked this post* https://t.co/3mAPNWNAfK\n",
            "...                                 ...                                                ...\n",
            "1315457129835302914     AnnieMoonIsBlue  RT @BillKristol: On President Trump and Dr. Fa...\n",
            "1315457130460327938        Dardo Boggia  RT @osvaldo_quiroga: Lo entrevisté muchísimas ...\n",
            "1315457129847889921                Shay    ISOLATION\\n\\nNot COVID. https://t.co/d4j0jm7j0e\n",
            "1315457128472088578           Amy Wolfe  RT @dockaurG: Political WHO's \"Science\"\\n\\nFeb...\n",
            "1315457130997088257          LuizFausto  RT @JamesTodaroMD: BREAKING: WHO reverses posi...\n",
            "\n",
            "[166536 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU2EE_-8e3Y0"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# pd.set_option(\"display.max_colwidth\", 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E990TDIcxo4j"
      },
      "source": [
        "# from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "# documents = dataset.data\n",
        "# len(documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-oiuIEgxo1Q"
      },
      "source": [
        "# dataset.target_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G6vVzk5yH15"
      },
      "source": [
        "# # news_df = pd.DataFrame({'document':documents})\n",
        "# news_df = pd.DataFrame(df)\n",
        "\n",
        "# # removing everything except alphabets`\n",
        "# news_df['clean_doc'] = news_df['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "\n",
        "# # removing short words\n",
        "# news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "\n",
        "# # make all text lowercase\n",
        "# news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4glglVaxyVXH"
      },
      "source": [
        "# import nltk\n",
        "# nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jio96rgyHyt"
      },
      "source": [
        "# from nltk.corpus import stopwords\n",
        "# stop_words = stopwords.words('english')\n",
        "\n",
        "# # tokenization\n",
        "# tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
        "\n",
        "# # remove stop-words\n",
        "# tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "\n",
        "# # de-tokenization\n",
        "# detokenized_doc = []\n",
        "# for i in range(len(news_df)):\n",
        "#     self.adj = {}\n",
        "#     t = ' '.join(tokenized_doc[i])\n",
        "#     detokenized_doc.append(t)\n",
        "\n",
        "# news_df['clean_doc'] = detokenized_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpWKWap07ciH"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBrpwgPg7cdx"
      },
      "source": [
        "def remove_links(text):\n",
        "    '''Takes a string and removes web links from it'''\n",
        "    text = re.sub(r'http\\S+', '', text) # remove http links\n",
        "    text = re.sub(r'bit.ly/\\S+', '', text) # rempve bitly links\n",
        "    text = text.strip('[link]') # remove [links]\n",
        "    return text\n",
        "\n",
        "def remove_users(text):\n",
        "    '''Takes a string and removes retweet and @user information'''\n",
        "    text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text) # remove retweet\n",
        "    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text) # remove tweeted at\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc7GcmMn7cbs",
        "outputId": "f1749054-4604-4e7a-9bf7-9837b0fe10d1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9dvBnP57cZq"
      },
      "source": [
        "my_stopwords = nltk.corpus.stopwords.words('english')\n",
        "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
        "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
        "\n",
        "# cleaning master function\n",
        "def clean_tweet(text, bigrams=False):\n",
        "    text = remove_users(text)\n",
        "    text = remove_links(text)\n",
        "    text = text.lower() # lower case\n",
        "    text = re.sub('['+my_punctuation + ']+', ' ', text) # strip punctuation\n",
        "    text = re.sub('\\s+', ' ', text) #remove double spacing\n",
        "    text = re.sub('([0-9]+)', '', text) # remove numbers\n",
        "    tweet_token_list = [word for word in text.split(' ')\n",
        "                            if word not in my_stopwords] # remove stopwords\n",
        "\n",
        "    tweet_token_list = [word_rooter(word) if '#' not in word else word\n",
        "                        for word in tweet_token_list] # apply word rooter\n",
        "    if bigrams:\n",
        "        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
        "                                            for i in range(len(tweet_token_list)-1)]\n",
        "    tweet = ' '.join(tweet_token_list)\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "JcGltmRL2UsR",
        "outputId": "edfed3d8-3d0f-478c-d4f7-c49b1db0eb99"
      },
      "source": [
        "lda['clean_tweet'] = lda.text.apply(clean_tweet)\n",
        "lda.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_name</th>\n",
              "      <th>text</th>\n",
              "      <th>clean_tweet</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1315442032651927552</th>\n",
              "      <td>Renee J North Lapp</td>\n",
              "      <td>@BalutSmithy @StephenJohn59 @jaketapper @kaitlancollins Seems to me that Fauci is also to blame for the mishandling of this pandemic, Birx too!  Pres listened to their advice of what to do, when t...</td>\n",
              "      <td>seem fauci also blame mishandl pandem birx pre listen advic pre doctor fauci led team blame fauci trust fauci chang opinion oft</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442033507684352</th>\n",
              "      <td>🎃 leh 🎃</td>\n",
              "      <td>RT @dougraz: todo dia pensando será se eu ja peguei covid ou não</td>\n",
              "      <td>todo dia pensando será se eu ja peguei covid ou não</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442037286678529</th>\n",
              "      <td>john coppa</td>\n",
              "      <td>RT @PetraCEsser1: @HSRetoucher Thank you. I am speaking here with tongues of angels.\\nThey bring a chip through BLOOD-BRAIN-BARRIER into the…</td>\n",
              "      <td>thank speak tongu angel bring chip blood brain barrier the…</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442040172425216</th>\n",
              "      <td>Reivindicación</td>\n",
              "      <td>@PedroAzzola https://t.co/RxL0DAaQaI</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442039870324736</th>\n",
              "      <td>rat zone</td>\n",
              "      <td>*corona liked this post* https://t.co/3mAPNWNAfK</td>\n",
              "      <td>corona like post</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442032350027780</th>\n",
              "      <td>ᴮᴱ#LAKERS 2020 NBA Champions #Titans (5-0)</td>\n",
              "      <td>https://t.co/VNNyjhzXxt https://t.co/DRIUE2K2VV</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442036028260353</th>\n",
              "      <td>tejakshi shetty</td>\n",
              "      <td>RT @SidNaaz13Dstny: Points\\n\\n1. Salman Sir #SidNaaz shipper h❤️\\n\\n2. Salman Sir Shehnaaz Biased Shipper h❤️\\n\\n3. Sidharth ko indirect way mai…</td>\n",
              "      <td>point  salman sir #sidnaaz shipper h❤️  salman sir shehnaaz bias shipper h❤️  sidharth ko indirect way mai…</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442038142390274</th>\n",
              "      <td>Angel Duarte Hurtado</td>\n",
              "      <td>RT @AristeguiOnline: Paga México más de 180 millones de dls. para garantizar vacuna contra Covid-19: SRE https://t.co/VaRI57CziU https://t.…</td>\n",
              "      <td>paga méxico má de  millon de dl para garantizar vacuna contra covid  sre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442033331507203</th>\n",
              "      <td>LoisR</td>\n",
              "      <td>RT @CLewandowski_: WHO now advising against lockdowns because they disproportionately hurt the poor—which is what ⁦@realDonaldTrump⁩ argued…</td>\n",
              "      <td>advis lockdown disproportion hurt poor—which ⁦⁩ argued…</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442037685194755</th>\n",
              "      <td>Jen Hensley</td>\n",
              "      <td>RT @RBReich: So let me get this straight: wearing a mask to save lives during a  pandemic should be a personal choice, but what a woman doe…</td>\n",
              "      <td>let get straight wear mask save live pandem person choic woman doe…</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      user_name  ...                                                                                                                       clean_tweet\n",
              "id                                                               ...                                                                                                                                  \n",
              "1315442032651927552                          Renee J North Lapp  ...   seem fauci also blame mishandl pandem birx pre listen advic pre doctor fauci led team blame fauci trust fauci chang opinion oft\n",
              "1315442033507684352                                     🎃 leh 🎃  ...                                                                               todo dia pensando será se eu ja peguei covid ou não\n",
              "1315442037286678529                                  john coppa  ...                                                                       thank speak tongu angel bring chip blood brain barrier the…\n",
              "1315442040172425216                              Reivindicación  ...                                                                                                                                  \n",
              "1315442039870324736                                    rat zone  ...                                                                                                                 corona like post \n",
              "1315442032350027780  ᴮᴱ#LAKERS 2020 NBA Champions #Titans (5-0)  ...                                                                                                                                  \n",
              "1315442036028260353                             tejakshi shetty  ...                       point  salman sir #sidnaaz shipper h❤️  salman sir shehnaaz bias shipper h❤️  sidharth ko indirect way mai…\n",
              "1315442038142390274                        Angel Duarte Hurtado  ...                                                         paga méxico má de  millon de dl para garantizar vacuna contra covid  sre \n",
              "1315442033331507203                                       LoisR  ...                                                                           advis lockdown disproportion hurt poor—which ⁦⁩ argued…\n",
              "1315442037685194755                                 Jen Hensley  ...                                                               let get straight wear mask save live pandem person choic woman doe…\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWAdV1RqyHwQ",
        "outputId": "4792b6fb-0d6b-447c-eb22-9234e13e6e0c"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', \n",
        "max_features= 1000, # keep top 1000 terms \n",
        "max_df = 0.5, \n",
        "smooth_idf=True)\n",
        "\n",
        "X = vectorizer.fit_transform(lda['clean_tweet'])\n",
        "\n",
        "X.shape # check shape of the document-term matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(166536, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxIraRcNyHua",
        "outputId": "4a96340d-51c8-4499-c87f-b0cb17388143"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# SVD represent documents and terms in vectors \n",
        "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
        "\n",
        "svd_model.fit(X)\n",
        "\n",
        "len(svd_model.components_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp15LNVRyHsW",
        "outputId": "b14a81b2-7e08-45fe-9dbf-6b2bdc6332b4"
      },
      "source": [
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "for i, comp in enumerate(svd_model.components_):\n",
        "    terms_comp = zip(terms, comp)\n",
        "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
        "    print(\"Topic \"+str(i)+\": \")\n",
        "    for t in sorted_terms:\n",
        "        print(t[0])\n",
        "        print(\" \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 0: \n",
            "like\n",
            " \n",
            "know\n",
            " \n",
            "people\n",
            " \n",
            "think\n",
            " \n",
            "good\n",
            " \n",
            "time\n",
            " \n",
            "thanks\n",
            " \n",
            "Topic 1: \n",
            "thanks\n",
            " \n",
            "windows\n",
            " \n",
            "card\n",
            " \n",
            "drive\n",
            " \n",
            "mail\n",
            " \n",
            "file\n",
            " \n",
            "advance\n",
            " \n",
            "Topic 2: \n",
            "game\n",
            " \n",
            "team\n",
            " \n",
            "year\n",
            " \n",
            "games\n",
            " \n",
            "season\n",
            " \n",
            "players\n",
            " \n",
            "good\n",
            " \n",
            "Topic 3: \n",
            "drive\n",
            " \n",
            "scsi\n",
            " \n",
            "disk\n",
            " \n",
            "hard\n",
            " \n",
            "card\n",
            " \n",
            "drives\n",
            " \n",
            "problem\n",
            " \n",
            "Topic 4: \n",
            "windows\n",
            " \n",
            "file\n",
            " \n",
            "window\n",
            " \n",
            "files\n",
            " \n",
            "program\n",
            " \n",
            "using\n",
            " \n",
            "problem\n",
            " \n",
            "Topic 5: \n",
            "government\n",
            " \n",
            "chip\n",
            " \n",
            "mail\n",
            " \n",
            "space\n",
            " \n",
            "information\n",
            " \n",
            "encryption\n",
            " \n",
            "data\n",
            " \n",
            "Topic 6: \n",
            "like\n",
            " \n",
            "bike\n",
            " \n",
            "know\n",
            " \n",
            "chip\n",
            " \n",
            "sounds\n",
            " \n",
            "looks\n",
            " \n",
            "look\n",
            " \n",
            "Topic 7: \n",
            "card\n",
            " \n",
            "sale\n",
            " \n",
            "video\n",
            " \n",
            "offer\n",
            " \n",
            "monitor\n",
            " \n",
            "price\n",
            " \n",
            "jesus\n",
            " \n",
            "Topic 8: \n",
            "know\n",
            " \n",
            "card\n",
            " \n",
            "chip\n",
            " \n",
            "video\n",
            " \n",
            "government\n",
            " \n",
            "people\n",
            " \n",
            "clipper\n",
            " \n",
            "Topic 9: \n",
            "good\n",
            " \n",
            "know\n",
            " \n",
            "time\n",
            " \n",
            "bike\n",
            " \n",
            "jesus\n",
            " \n",
            "problem\n",
            " \n",
            "work\n",
            " \n",
            "Topic 10: \n",
            "think\n",
            " \n",
            "chip\n",
            " \n",
            "good\n",
            " \n",
            "thanks\n",
            " \n",
            "clipper\n",
            " \n",
            "need\n",
            " \n",
            "encryption\n",
            " \n",
            "Topic 11: \n",
            "thanks\n",
            " \n",
            "right\n",
            " \n",
            "problem\n",
            " \n",
            "good\n",
            " \n",
            "bike\n",
            " \n",
            "time\n",
            " \n",
            "window\n",
            " \n",
            "Topic 12: \n",
            "good\n",
            " \n",
            "people\n",
            " \n",
            "windows\n",
            " \n",
            "know\n",
            " \n",
            "file\n",
            " \n",
            "sale\n",
            " \n",
            "files\n",
            " \n",
            "Topic 13: \n",
            "space\n",
            " \n",
            "think\n",
            " \n",
            "know\n",
            " \n",
            "nasa\n",
            " \n",
            "problem\n",
            " \n",
            "year\n",
            " \n",
            "israel\n",
            " \n",
            "Topic 14: \n",
            "space\n",
            " \n",
            "good\n",
            " \n",
            "card\n",
            " \n",
            "people\n",
            " \n",
            "time\n",
            " \n",
            "nasa\n",
            " \n",
            "thanks\n",
            " \n",
            "Topic 15: \n",
            "people\n",
            " \n",
            "problem\n",
            " \n",
            "window\n",
            " \n",
            "time\n",
            " \n",
            "game\n",
            " \n",
            "want\n",
            " \n",
            "bike\n",
            " \n",
            "Topic 16: \n",
            "time\n",
            " \n",
            "bike\n",
            " \n",
            "right\n",
            " \n",
            "windows\n",
            " \n",
            "file\n",
            " \n",
            "need\n",
            " \n",
            "really\n",
            " \n",
            "Topic 17: \n",
            "time\n",
            " \n",
            "problem\n",
            " \n",
            "file\n",
            " \n",
            "think\n",
            " \n",
            "israel\n",
            " \n",
            "long\n",
            " \n",
            "mail\n",
            " \n",
            "Topic 18: \n",
            "file\n",
            " \n",
            "need\n",
            " \n",
            "card\n",
            " \n",
            "files\n",
            " \n",
            "problem\n",
            " \n",
            "right\n",
            " \n",
            "good\n",
            " \n",
            "Topic 19: \n",
            "problem\n",
            " \n",
            "file\n",
            " \n",
            "thanks\n",
            " \n",
            "used\n",
            " \n",
            "space\n",
            " \n",
            "chip\n",
            " \n",
            "sale\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvPazOL_zP3o",
        "outputId": "fd0e33f9-f9ec-4154-8021-50b5ab5ae3e3"
      },
      "source": [
        "pip install 'umap-learn==0.3.10'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting umap-learn==0.3.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/92/36bac74962b424870026cb0b42cec3d5b6f4afa37d81818475d8762f9255/umap-learn-0.3.10.tar.gz (40kB)\n",
            "\r\u001b[K     |████████                        | 10kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 20.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 30kB 25.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.7/dist-packages (from umap-learn==0.3.10) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.7/dist-packages (from umap-learn==0.3.10) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from umap-learn==0.3.10) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.37 in /usr/local/lib/python3.7/dist-packages (from umap-learn==0.3.10) (0.51.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->umap-learn==0.3.10) (1.0.1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.37->umap-learn==0.3.10) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.37->umap-learn==0.3.10) (57.0.0)\n",
            "Building wheels for collected packages: umap-learn\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.3.10-cp37-none-any.whl size=38882 sha256=375f1805a7d8cc608f81ef9435c43627ef6f534095b8babbbd6d14e4443a6945\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/f8/d5/8e3af3ee957feb9b403a060ebe72f7561887fef9dea658326e\n",
            "Successfully built umap-learn\n",
            "Installing collected packages: umap-learn\n",
            "Successfully installed umap-learn-0.3.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZzNgsMrxoy1"
      },
      "source": [
        "import umap\n",
        "\n",
        "X_topics = svd_model.fit_transform(X)\n",
        "embedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(X_topics)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1], \n",
        "c = dataset.target,\n",
        "s = 10, # size\n",
        "edgecolor='none'\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dH4G5Zmxowl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ou1S5Tyxota"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7XX4HFDxorQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vzpoIj2Z_DJ"
      },
      "source": [
        "def remove_links(text):\n",
        "    '''Takes a string and removes web links from it'''\n",
        "    text = re.sub(r'http\\S+', '', text) # remove http links\n",
        "    text = re.sub(r'bit.ly/\\S+', '', text) # rempve bitly links\n",
        "    text = text.strip('[link]') # remove [links]\n",
        "    return text\n",
        "\n",
        "def remove_users(text):\n",
        "    '''Takes a string and removes retweet and @user information'''\n",
        "    text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text) # remove retweet\n",
        "    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text) # remove tweeted at\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrDi6ZOliqeq",
        "outputId": "a8d1b061-365c-41c6-c546-850dceb5cd64"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e0hRe_DZ_B9"
      },
      "source": [
        "my_stopwords = nltk.corpus.stopwords.words('english')\n",
        "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
        "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
        "\n",
        "# cleaning master function\n",
        "def clean_tweet(text, bigrams=False):\n",
        "    text = remove_users(text)\n",
        "    text = remove_links(text)\n",
        "    text = text.lower() # lower case\n",
        "    text = re.sub('['+my_punctuation + ']+', ' ', text) # strip punctuation\n",
        "    text = re.sub('\\s+', ' ', text) #remove double spacing\n",
        "    text = re.sub('([0-9]+)', '', text) # remove numbers\n",
        "    tweet_token_list = [word for word in text.split(' ')\n",
        "                            if word not in my_stopwords] # remove stopwords\n",
        "\n",
        "    tweet_token_list = [word_rooter(word) if '#' not in word else word\n",
        "                        for word in tweet_token_list] # apply word rooter\n",
        "    if bigrams:\n",
        "        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
        "                                            for i in range(len(tweet_token_list)-1)]\n",
        "    tweet = ' '.join(tweet_token_list)\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "NlQVO6XdZ_A4",
        "outputId": "7896c17a-3ae8-455f-bcf1-4924b522f1a1"
      },
      "source": [
        "lda['clean_tweet'] = lda.text.apply(clean_tweet)\n",
        "lda.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_name</th>\n",
              "      <th>text</th>\n",
              "      <th>clean_tweet</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1315442032651927552</th>\n",
              "      <td>Renee J North Lapp</td>\n",
              "      <td>@BalutSmithy @StephenJohn59 @jaketapper @kaitl...</td>\n",
              "      <td>seem fauci also blame mishandl pandem birx pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442033507684352</th>\n",
              "      <td>🎃 leh 🎃</td>\n",
              "      <td>RT @dougraz: todo dia pensando será se eu ja p...</td>\n",
              "      <td>todo dia pensando será se eu ja peguei covid ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442037286678529</th>\n",
              "      <td>john coppa</td>\n",
              "      <td>RT @PetraCEsser1: @HSRetoucher Thank you. I am...</td>\n",
              "      <td>thank speak tongu angel bring chip blood brai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442040172425216</th>\n",
              "      <td>Reivindicación</td>\n",
              "      <td>@PedroAzzola https://t.co/RxL0DAaQaI</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442039870324736</th>\n",
              "      <td>rat zone</td>\n",
              "      <td>*corona liked this post* https://t.co/3mAPNWNAfK</td>\n",
              "      <td>corona like post</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442032350027780</th>\n",
              "      <td>ᴮᴱ#LAKERS 2020 NBA Champions #Titans (5-0)</td>\n",
              "      <td>https://t.co/VNNyjhzXxt https://t.co/DRIUE2K2VV</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442036028260353</th>\n",
              "      <td>tejakshi shetty</td>\n",
              "      <td>RT @SidNaaz13Dstny: Points\\n\\n1. Salman Sir #S...</td>\n",
              "      <td>point  salman sir #sidnaaz shipper h❤️  salma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442038142390274</th>\n",
              "      <td>Angel Duarte Hurtado</td>\n",
              "      <td>RT @AristeguiOnline: Paga México más de 180 mi...</td>\n",
              "      <td>paga méxico má de  millon de dl para garantiz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442033331507203</th>\n",
              "      <td>LoisR</td>\n",
              "      <td>RT @CLewandowski_: WHO now advising against lo...</td>\n",
              "      <td>advis lockdown disproportion hurt poor—which ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315442037685194755</th>\n",
              "      <td>Jen Hensley</td>\n",
              "      <td>RT @RBReich: So let me get this straight: wear...</td>\n",
              "      <td>let get straight wear mask save live pandem p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      user_name  ...                                        clean_tweet\n",
              "id                                                               ...                                                   \n",
              "1315442032651927552                          Renee J North Lapp  ...   seem fauci also blame mishandl pandem birx pr...\n",
              "1315442033507684352                                     🎃 leh 🎃  ...   todo dia pensando será se eu ja peguei covid ...\n",
              "1315442037286678529                                  john coppa  ...   thank speak tongu angel bring chip blood brai...\n",
              "1315442040172425216                              Reivindicación  ...                                                   \n",
              "1315442039870324736                                    rat zone  ...                                  corona like post \n",
              "1315442032350027780  ᴮᴱ#LAKERS 2020 NBA Champions #Titans (5-0)  ...                                                   \n",
              "1315442036028260353                             tejakshi shetty  ...   point  salman sir #sidnaaz shipper h❤️  salma...\n",
              "1315442038142390274                        Angel Duarte Hurtado  ...   paga méxico má de  millon de dl para garantiz...\n",
              "1315442033331507203                                       LoisR  ...   advis lockdown disproportion hurt poor—which ...\n",
              "1315442037685194755                                 Jen Hensley  ...   let get straight wear mask save live pandem p...\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7YYHH64Z-7E"
      },
      "source": [
        "# Applying Topic Modelling\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# the vectorizer object will be used to transform text to vector form\n",
        "vectorizer = CountVectorizer(max_df=0.9, min_df=25, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
        "\n",
        "# apply transformation\n",
        "tf = vectorizer.fit_transform(lda['clean_tweet']).toarray()\n",
        "\n",
        "# tf_feature_names tells us what word each column in the matric represents\n",
        "tf_feature_names = vectorizer.get_feature_names()\n",
        "# tf_feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujt-K251jYr6"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "number_of_topics = 10\n",
        "\n",
        "model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOaJvkDTjYog",
        "outputId": "551abea4-a815-47cf-ce59-9f29df15b883"
      },
      "source": [
        "model.fit(tf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
              "                          evaluate_every=-1, learning_decay=0.7,\n",
              "                          learning_method='batch', learning_offset=10.0,\n",
              "                          max_doc_update_iter=100, max_iter=10,\n",
              "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
              "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
              "                          total_samples=1000000.0, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3Q4XSUmjYnA"
      },
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "    topic_dict = {}\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
        "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
        "    return pd.DataFrame(topic_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "fO3Bz_ZsjYkL",
        "outputId": "2b0a5cf3-2da0-493a-d5e6-e074903f75b5"
      },
      "source": [
        "no_top_words = 10\n",
        "display_topics(model, tf_feature_names, no_top_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic 0 words</th>\n",
              "      <th>Topic 0 weights</th>\n",
              "      <th>Topic 1 words</th>\n",
              "      <th>Topic 1 weights</th>\n",
              "      <th>Topic 2 words</th>\n",
              "      <th>Topic 2 weights</th>\n",
              "      <th>Topic 3 words</th>\n",
              "      <th>Topic 3 weights</th>\n",
              "      <th>Topic 4 words</th>\n",
              "      <th>Topic 4 weights</th>\n",
              "      <th>Topic 5 words</th>\n",
              "      <th>Topic 5 weights</th>\n",
              "      <th>Topic 6 words</th>\n",
              "      <th>Topic 6 weights</th>\n",
              "      <th>Topic 7 words</th>\n",
              "      <th>Topic 7 weights</th>\n",
              "      <th>Topic 8 words</th>\n",
              "      <th>Topic 8 weights</th>\n",
              "      <th>Topic 9 words</th>\n",
              "      <th>Topic 9 weights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>…</td>\n",
              "      <td>8684.6</td>\n",
              "      <td>pandem</td>\n",
              "      <td>7489.9</td>\n",
              "      <td>…</td>\n",
              "      <td>5993.9</td>\n",
              "      <td>…</td>\n",
              "      <td>4205.1</td>\n",
              "      <td>…</td>\n",
              "      <td>13692.0</td>\n",
              "      <td>…</td>\n",
              "      <td>4977.5</td>\n",
              "      <td>de</td>\n",
              "      <td>19647.1</td>\n",
              "      <td>covid</td>\n",
              "      <td>10327.5</td>\n",
              "      <td>…</td>\n",
              "      <td>13180.9</td>\n",
              "      <td>…</td>\n",
              "      <td>12499.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>de</td>\n",
              "      <td>5287.7</td>\n",
              "      <td>go</td>\n",
              "      <td>5215.8</td>\n",
              "      <td>lockdown</td>\n",
              "      <td>2808.7</td>\n",
              "      <td>trump</td>\n",
              "      <td>3540.0</td>\n",
              "      <td>covid</td>\n",
              "      <td>13658.5</td>\n",
              "      <td>de</td>\n",
              "      <td>2061.3</td>\n",
              "      <td>la</td>\n",
              "      <td>12528.0</td>\n",
              "      <td>…</td>\n",
              "      <td>8515.6</td>\n",
              "      <td>covid</td>\n",
              "      <td>9242.6</td>\n",
              "      <td>mask</td>\n",
              "      <td>8518.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>comment</td>\n",
              "      <td>3803.1</td>\n",
              "      <td>covid</td>\n",
              "      <td>3730.4</td>\n",
              "      <td>end</td>\n",
              "      <td>1752.9</td>\n",
              "      <td>fauci</td>\n",
              "      <td>3324.2</td>\n",
              "      <td>’</td>\n",
              "      <td>5297.1</td>\n",
              "      <td>lockdown</td>\n",
              "      <td>1882.5</td>\n",
              "      <td>el</td>\n",
              "      <td>10200.0</td>\n",
              "      <td>new</td>\n",
              "      <td>3406.1</td>\n",
              "      <td>trump</td>\n",
              "      <td>7783.7</td>\n",
              "      <td>wear</td>\n",
              "      <td>8320.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>amp</td>\n",
              "      <td>3731.8</td>\n",
              "      <td>y</td>\n",
              "      <td>3490.1</td>\n",
              "      <td>la</td>\n",
              "      <td>1617.4</td>\n",
              "      <td>respons</td>\n",
              "      <td>2727.1</td>\n",
              "      <td>’t</td>\n",
              "      <td>4764.4</td>\n",
              "      <td>covid</td>\n",
              "      <td>1851.8</td>\n",
              "      <td>covid</td>\n",
              "      <td>9239.6</td>\n",
              "      <td>trump</td>\n",
              "      <td>3403.1</td>\n",
              "      <td>’</td>\n",
              "      <td>7099.2</td>\n",
              "      <td>live</td>\n",
              "      <td>6440.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>en</td>\n",
              "      <td>3725.1</td>\n",
              "      <td>hear</td>\n",
              "      <td>3462.0</td>\n",
              "      <td>call</td>\n",
              "      <td>1452.4</td>\n",
              "      <td>say</td>\n",
              "      <td>2693.7</td>\n",
              "      <td>peopl</td>\n",
              "      <td>3890.5</td>\n",
              "      <td>e</td>\n",
              "      <td>1712.1</td>\n",
              "      <td>…</td>\n",
              "      <td>8948.0</td>\n",
              "      <td>american</td>\n",
              "      <td>3316.3</td>\n",
              "      <td>twitter</td>\n",
              "      <td>3011.7</td>\n",
              "      <td>get</td>\n",
              "      <td>5534.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>campaign</td>\n",
              "      <td>3665.6</td>\n",
              "      <td>’all</td>\n",
              "      <td>3364.1</td>\n",
              "      <td>cdc</td>\n",
              "      <td>1429.4</td>\n",
              "      <td>dr</td>\n",
              "      <td>2419.9</td>\n",
              "      <td>pandem</td>\n",
              "      <td>3378.3</td>\n",
              "      <td>que</td>\n",
              "      <td>1696.8</td>\n",
              "      <td>que</td>\n",
              "      <td>8931.3</td>\n",
              "      <td>death</td>\n",
              "      <td>3238.1</td>\n",
              "      <td>test</td>\n",
              "      <td>2868.3</td>\n",
              "      <td>let</td>\n",
              "      <td>5444.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>latest</td>\n",
              "      <td>3548.1</td>\n",
              "      <td>song</td>\n",
              "      <td>3288.1</td>\n",
              "      <td>sign</td>\n",
              "      <td>1426.3</td>\n",
              "      <td>ad</td>\n",
              "      <td>2344.8</td>\n",
              "      <td>like</td>\n",
              "      <td>3353.6</td>\n",
              "      <td>current</td>\n",
              "      <td>1599.5</td>\n",
              "      <td>en</td>\n",
              "      <td>6718.1</td>\n",
              "      <td>die</td>\n",
              "      <td>3231.9</td>\n",
              "      <td>he</td>\n",
              "      <td>2856.1</td>\n",
              "      <td>person</td>\n",
              "      <td>5308.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>fauci</td>\n",
              "      <td>3491.5</td>\n",
              "      <td>…</td>\n",
              "      <td>2490.4</td>\n",
              "      <td>coronaviru</td>\n",
              "      <td>1357.5</td>\n",
              "      <td>coronaviru</td>\n",
              "      <td>2192.2</td>\n",
              "      <td>get</td>\n",
              "      <td>3315.6</td>\n",
              "      <td>com</td>\n",
              "      <td>1553.7</td>\n",
              "      <td>por</td>\n",
              "      <td>4229.1</td>\n",
              "      <td>case</td>\n",
              "      <td>2977.2</td>\n",
              "      <td>dr</td>\n",
              "      <td>2705.2</td>\n",
              "      <td>pandem</td>\n",
              "      <td>5113.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>use</td>\n",
              "      <td>3445.2</td>\n",
              "      <td>flu</td>\n",
              "      <td>2030.1</td>\n",
              "      <td>scientist</td>\n",
              "      <td>1257.8</td>\n",
              "      <td>new</td>\n",
              "      <td>1731.6</td>\n",
              "      <td>trump</td>\n",
              "      <td>3244.4</td>\n",
              "      <td>uma</td>\n",
              "      <td>1513.1</td>\n",
              "      <td>lo</td>\n",
              "      <td>4081.5</td>\n",
              "      <td>day</td>\n",
              "      <td>2647.0</td>\n",
              "      <td>immun</td>\n",
              "      <td>2514.2</td>\n",
              "      <td>save</td>\n",
              "      <td>4929.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>dr</td>\n",
              "      <td>3443.5</td>\n",
              "      <td>tell</td>\n",
              "      <td>2020.5</td>\n",
              "      <td>chri</td>\n",
              "      <td>1152.1</td>\n",
              "      <td>campaign</td>\n",
              "      <td>1724.5</td>\n",
              "      <td>amp</td>\n",
              "      <td>2961.3</td>\n",
              "      <td>pode</td>\n",
              "      <td>1483.1</td>\n",
              "      <td>se</td>\n",
              "      <td>3793.2</td>\n",
              "      <td>one</td>\n",
              "      <td>2384.9</td>\n",
              "      <td>coronaviru</td>\n",
              "      <td>2472.9</td>\n",
              "      <td>choic</td>\n",
              "      <td>4434.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Topic 0 words Topic 0 weights  ... Topic 9 words Topic 9 weights\n",
              "0             …          8684.6  ...             …         12499.0\n",
              "1            de          5287.7  ...          mask          8518.4\n",
              "2       comment          3803.1  ...          wear          8320.0\n",
              "3           amp          3731.8  ...          live          6440.9\n",
              "4            en          3725.1  ...           get          5534.1\n",
              "5      campaign          3665.6  ...           let          5444.3\n",
              "6        latest          3548.1  ...        person          5308.0\n",
              "7         fauci          3491.5  ...        pandem          5113.0\n",
              "8           use          3445.2  ...          save          4929.1\n",
              "9            dr          3443.5  ...         choic          4434.1\n",
              "\n",
              "[10 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNnXFQX0jYiV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeESafVgjYgf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwzZDWdyjYeT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rYUMc-1jYcZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrMuZywJjYae"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t4QyinzE2h7"
      },
      "source": [
        "# df['is_retweet'] = df['text'].apply(lambda x: x[:2]=='RT')\n",
        "# df['is_retweet'].sum()  # number of retweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0myOB7IFvVx"
      },
      "source": [
        "# df.groupby(['text']).size().reset_index(name='counts')\\\n",
        "#   .sort_values('counts', ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aucn7FgwF4KO"
      },
      "source": [
        "# tweets = pd.DataFrame(df)\n",
        "# def find_mentioned(tweet):\n",
        "#     '''This function will extract the twitter handles of people mentioned in the tweet'''\n",
        "#     return re.findall('(?<!RT\\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', tweet)\n",
        "# def find_hashtags(tweet):\n",
        "#     '''This function will extract hashtags'''\n",
        "#     return re.findall('(#[A-Za-z]+[A-Za-z0-9-_]+)', tweet)\n",
        "# # make new columns for mentioned usernames and hashtags\n",
        "# tweets['mentioned'] = tweets.text.apply(find_mentioned)\n",
        "# tweets['hashtags'] = tweets.text.apply(find_hashtags)\n",
        "# tweets.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE_y5JzxGy-t"
      },
      "source": [
        "# # number of unique hashtags\n",
        "# flattened_hashtags_df = pd.DataFrame(tweets)\n",
        "# flattened_hashtags_df['hashtags'].unique().size\n",
        "# flattened_hashtags_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYXpcGvPJHA-"
      },
      "source": [
        "# popular_hashtags = list(log['hashtags'])\n",
        "# popular_hashtags = [x for x in all_tweets if str(x) != 'nan']\n",
        "\n",
        "# popular_hashtags[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeZo_smdJuY8"
      },
      "source": [
        "# popular_hash['hashtag'].unique().size\n",
        "# popular_hashtags[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ltP2h3JLMz"
      },
      "source": [
        "# # take hashtags which appear at least this amount of times\n",
        "# min_appearance = 1000\n",
        "# # find popular hashtags - make into python set for efficiency\n",
        "# popular_hashtags_set = set(popular_hashtags[popular_hashtags.counts>=min_appearance]['hashtags'])\n",
        "# popular_hashtags_set[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAer1gZ-WKBp"
      },
      "source": [
        "# my_stopwords = nltk.corpus.stopwords.words('english')\n",
        "# word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
        "# my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
        "# # cleaning master function\n",
        "# def clean_tweet(tweet, bigrams=False):\n",
        "#     tweet = remove_users(tweet)\n",
        "#     tweet = remove_links(tweet)\n",
        "#     tweet = tweet.lower() # lower case\n",
        "#     tweet = re.sub('['+my_punctuation + ']+', ' ', tweet) # strip punctuation\n",
        "#     tweet = re.sub('\\s+', ' ', tweet) #remove double spacing\n",
        "#     tweet = re.sub('([0-9]+)', '', tweet) # remove numbers\n",
        "#     tweet_token_list = [word for word in tweet.split(' ')\n",
        "#                             if word not in my_stopwords] # remove stopwords\n",
        "# tweet_token_list = [word_rooter(word) if '#' not in word else word\n",
        "#                         for word in tweet_token_list] # apply word rooter\n",
        "# if bigrams:\n",
        "#         tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
        "#                                             for i in range(len(tweet_token_list)-1)]\n",
        "#     tweet = ' '.join(tweet_token_list)\n",
        "#     return tweet\n",
        "# tweets['clean_tweet'] = tweets.full_text.apply(clean_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H6jA68mW5cy",
        "outputId": "8d7162c8-502b-42c3-856a-b523ba5d6d24"
      },
      "source": [
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# # the vectorizer object will be used to transform text to vector form\n",
        "# vectorizer = CountVectorizer(max_df=0.9, min_df=100, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
        "# # apply transformation\n",
        "# tf = vectorizer.fit_transform(tweets['text']) #.toarray()\n",
        "# # tf_feature_names tells us what word each column in the matric represents\n",
        "# tf_feature_names = vectorizer.get_feature_names()\n",
        "# tf.shape # --> (200000, 2296)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(166536, 3152)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOPSFFZiXCqv",
        "outputId": "04193f4a-a9ee-4701-e49b-0b52eb89c7c9"
      },
      "source": [
        "# from sklearn.decomposition import LatentDirichletAllocation\n",
        "# number_of_topics = 10\n",
        "# model = LatentDirichletAllocation(n_components=number_of_topics, random_state=45) # random state for reproducibility\n",
        "# # Fit data to model\n",
        "# model.fit(tf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
              "                          evaluate_every=-1, learning_decay=0.7,\n",
              "                          learning_method='batch', learning_offset=10.0,\n",
              "                          max_doc_update_iter=100, max_iter=10,\n",
              "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
              "                          perp_tol=0.1, random_state=45, topic_word_prior=None,\n",
              "                          total_samples=1000000.0, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}